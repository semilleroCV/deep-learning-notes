{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![deep-learning-notes](https://github.com/semilleroCV/deep-learning-notes/raw/main/assets/banner-notebook.png)](https://github.com/semilleroCV/deep-learning-notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title **install required packages**\n",
    "! pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **import required libraries**\n",
    "import torch # 2.2.1\n",
    "import torch.nn as nn # 2.2.1\n",
    "from torchinfo import summary # 1.8.0\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order not to saturate with a lot of text, each of the modules are explained just before the class implementation. But the general idea of the model pipeline is the following:\n",
    "\n",
    "1. Convert input sentences to embeddings.\n",
    "2. Add positional encoding to embeddings.\n",
    "3. Process through the encoder with multi-head self-attention and feed-forward layers, with residual connections and layer normalization.\n",
    "4. Decoder processes the encoded output and the target sentence with added masked self-attention.\n",
    "5. Apply linear projection.\n",
    "\n",
    "Comments: Softmax is omitted for flexibility, embeddings are scaled by âˆšdmodel, and dropout (rate 0.1) is applied to sub-layer outputs, embeddings, and positional encodings.\n",
    "\n",
    "![overview.png](https://github.com/user-attachments/assets/6504bae8-1fc5-4873-9193-d3dcd734ba9c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations 1 and 2\n",
    "\n",
    "1. Here, each sentence has already been converted into embeddings, this is done in the `Transformer` class with the `nn.Embedding` module.\n",
    "2. Then, a encoded position (`PositionalEncoding`) is added to each of the sentence embeddings, this is done because the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. \n",
    "\n",
    "We can visualize what is happening with the following example:\n",
    "\n",
    "![positional_encoding](https://github.com/user-attachments/assets/8b014338-b18f-4b8d-9d9a-bc5042cc1ff5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  \"\"\"\n",
    "  Some Information about PositionalEncoding\n",
    "\n",
    "  d_model: The size of the word embeddings to be used.\n",
    "  max_seq_len: The maximum number of tokens the model can\n",
    "  receive, here is useful to know how many functions to generate.\n",
    "  \"\"\"\n",
    "  def __init__(self, max_seq_len:int=128, d_model:int=512, device:str='cpu'):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.positional_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "    \n",
    "    token_position = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "    \n",
    "    div_term = torch.exp(2*torch.arange(0, d_model, 1, dtype = torch.float) * -torch.log(torch.tensor(10000)) / d_model)\n",
    "    # div_term = torch.exp(torch.arange(0, d_model, 2, dtype = torch.float) * -torch.log(torch.tensor(10000)) / d_model)\n",
    "    \n",
    "    self.positional_matrix[:, 0::2] = torch.sin(token_position*div_term[0::2])\n",
    "    self.positional_matrix[:, 1::2] = torch.cos(token_position*div_term[1::2])\n",
    "    # self.positional_matrix[:, 1::2] = torch.cos(token_position*div_term)\n",
    "    # self.positional_matrix[:, 1::2] = torch.cos(token_position*div_term)\n",
    "    self.positional_matrix = self.positional_matrix.unsqueeze(0).transpose(0, 1)\n",
    "    # print(self.positional_matrix, self.positional_matrix.size())\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.positional_matrix[:x.size(0), :] # we only add up to the number of tokens that come in the sequence, not the rest because it will be padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2920,  0.8996, -0.5995, -1.2486],\n",
      "         [-0.5859,  0.1799,  1.0557, -0.7846],\n",
      "         [ 1.2099,  0.2954, -0.2674,  0.5775]],\n",
      "\n",
      "        [[ 0.3912,  0.1120,  0.3118,  1.0472],\n",
      "         [ 0.1246, -1.3737, -1.1956,  1.6167],\n",
      "         [-0.4745,  1.9012, -0.1252, -0.0335]]]) torch.Size([2, 3, 4])\n",
      "tensor([[[ 0.2920,  1.8996, -0.5995, -0.2486],\n",
      "         [-0.5859,  1.1799,  1.0557,  0.2154],\n",
      "         [ 1.2099,  1.2954, -0.2674,  1.5775]],\n",
      "\n",
      "        [[ 1.2326,  1.1119,  0.3119,  2.0472],\n",
      "         [ 0.9661, -0.3738, -1.1955,  2.6167],\n",
      "         [ 0.3670,  2.9012, -0.1251,  0.9665]]]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test the module to see if it gives the expected result.\n",
    "\n",
    "pos = PositionalEncoding(5, 4)\n",
    "source = torch.randn(2, 3, 4)\n",
    "print(source, source.size())\n",
    "source = pos(source)\n",
    "print(source, source.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\"\n",
    "  for a detailed explanation of MultiHeadAttention\n",
    "  refer to https://github.com/semilleroCV/deep-learning-notes/blob/main/notebooks/modules/multi-head-attention.ipynb\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, d_model:int = 512, num_heads:int = 8):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    \n",
    "    # We make sure that the division is integer.\n",
    "    assert d_model % num_heads == 0, \"Size not compatible with number of heads\"\n",
    "\n",
    "    self.num_heads = num_heads\n",
    "    self.d_v = d_model//num_heads\n",
    "    self.d_k = self.d_v\n",
    "\n",
    "    # In order not to have to make h matrices we will \"extend\" them from d_k to d_model\n",
    "    # that is, d_k, h times.\n",
    "    self.W_q = nn.Linear(d_model, self.d_k * num_heads)\n",
    "    self.W_k = nn.Linear(d_model, self.d_k * num_heads)\n",
    "    self.W_v = nn.Linear(d_model, self.d_k * num_heads)\n",
    "    self.W_o = nn.Linear(self.d_v * num_heads, d_model)\n",
    "\n",
    "  def forward(self, Q, K, V, mask = None):\n",
    "\n",
    "    batch_size = Q.size(0)\n",
    "\n",
    "    Q, K, V = self.W_q(Q), self.W_k(K), self.W_v(V)\n",
    "    Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # here we are \"deconcatenating\" the Querys for each head.\n",
    "    K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    V = V.view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "    \"\"\" \n",
    "      Q, K and V -> [batch_size, seq_len, d_k * num_heads]\n",
    "      after transpose Q, K and V -> [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "\n",
    "    weighted_values, attention = self.scaled_dot_pruduct_attention(Q, K, V, mask)\n",
    "\n",
    "    # contiguous() will rearrange the memory allocation so that the tensor is C contiguous https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch\n",
    "    weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k) # we have to concatenate the output of the scaled_dot_product\n",
    "\n",
    "    \"\"\"\n",
    "    weighted_values -> [batch_size, num_heads, seq_len, d_k]\n",
    "    after transpose weighted_values -> [batch_size, seq_len, num_heads * d_k]\n",
    "    \"\"\"\n",
    "\n",
    "    weighted_values = self.W_o(weighted_values) # [batch_size, seq_len, d_model]\n",
    "\n",
    "    return weighted_values, attention\n",
    "  \n",
    "  def scaled_dot_pruduct_attention(self, Q, K, V, mask = None):\n",
    "      \n",
    "      # matmul operates for each head\n",
    "      compatibility = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # the transpose here is, as in the paper, so that the multiplication between matrices can be done\n",
    "\n",
    "      if mask is not None:\n",
    "         compatibility = compatibility.masked_fill(mask == 0, -float('inf')) # pay attention only to the tokens we are interested in\n",
    "\n",
    "      attention = nn.Softmax(dim=-1)(compatibility) # dim=-1 is to be applied for each vector of q, k and v, [batch_size, num_heads, seq_len, seq_len]\n",
    "      weighted_values = torch.matmul(attention, V)\n",
    "      return weighted_values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0217, -0.5313, -0.3825,  0.6463],\n",
       "          [-0.1172, -0.6836, -0.5176,  0.7283]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[[0.3212, 0.6788],\n",
       "           [0.5680, 0.4320]],\n",
       " \n",
       "          [[0.3251, 0.6749],\n",
       "           [0.5950, 0.4050]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the module to see if it gives the expected result.\n",
    "\n",
    "d_model, seq_len, num_heads, d_k = 4, 2, 2, 2\n",
    "\n",
    "module = MultiHeadAttention(d_model, num_heads)\n",
    "sentence = torch.randn(1, seq_len, d_model)\n",
    "module(sentence, sentence, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class represents the Feed Forward submodules that are represented by the blue box in the architecture image.\n",
    "\n",
    "![position_wise_feed_forward_networks](https://github.com/user-attachments/assets/233237ec-f55c-4c76-89a4-b2dce8c94fa9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "  \"\"\"Some Information about PositionWiseFeedForward\"\"\"\n",
    "  def __init__(self, d_ff:int=2048, d_model:int=512):\n",
    "    super(PositionWiseFeedForward, self).__init__()\n",
    "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "    self.linear_2 = nn.Linear(d_ff, d_model) \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = nn.ReLU()(self.linear_1(x))\n",
    "    return self.linear_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "  \"\"\"Some Information about EncoderLayer\"\"\"\n",
    "  def __init__(self, d_model:int=512, d_ff:int=2048, \n",
    "               num_heads:int=8, dropout:int=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "    # dropout 1\n",
    "    self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_ff, d_model)\n",
    "    # dropout 2\n",
    "    self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x, mask = None):\n",
    "    # Multi-Head Attention and Dropout\n",
    "    attention_score, _ = self.multi_head_attention(x, x, x, mask)\n",
    "    attention_score = nn.Dropout(self.dropout)(attention_score)\n",
    "\n",
    "    # Add and Layer norm 1\n",
    "    x = self.layer_norm_1(x + attention_score)\n",
    "\n",
    "    # Feed Forward and Dropout \n",
    "    ff_output = nn.Dropout(self.dropout)(self.feed_forward(x))\n",
    "\n",
    "    # Add and Layer norm 2\n",
    "    x = self.layer_norm_2(x + ff_output)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation 3\n",
    "\n",
    "3. At this point, the source sentence first passes through the **Enconder**, which is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism (`MultiHeadAttention`), and the second is a simple, positionwise fully connected feed-forward network (`PositionWiseFeedForward`). We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \"\"\"\n",
    "  A Transformer encoder with multiple layers (`EncoderLayer`) that applies self-attention and feed-forward mechanisms. \n",
    "  Processes input sequences into encoded representations. Includes layer normalization for stability.\n",
    "\n",
    "  Args:\n",
    "  - d_model (int): Dimension of embeddings.\n",
    "  - n_layers (int): Number of encoder layers.\n",
    "  - d_ff (int): Feed-forward hidden layer size.\n",
    "  - num_heads (int): Attention heads.\n",
    "  - dropout (float): Dropout rate.\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model:int=512, n_layers:int=6, d_ff:int=2048, \n",
    "               num_heads:int=8, dropout:int=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.layers = nn.ModuleList(EncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(n_layers))\n",
    "    self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "  def forward(self, x, mask = None):\n",
    "    \"\"\"\n",
    "    The encoder is also masked so as not to attend to the padding tokens.\n",
    "    \"\"\"\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "    return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "  \"\"\"Some Information about DecoderLayer\"\"\"\n",
    "  def __init__(self, d_model:int=512, d_ff:int=2048, num_heads:int=8, dropout:int=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.multi_head_attention_1 = MultiHeadAttention(d_model, num_heads)\n",
    "    # dropout 1\n",
    "    self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "    self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "    # dropout 2\n",
    "    self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "    self.feed_forward = PositionWiseFeedForward(d_ff, d_model)\n",
    "    # dropout 3\n",
    "    self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x, encoder_output, encoder_mask = None, target_mask = None):\n",
    "\n",
    "    # Multi-Head attention and Dropout\n",
    "    attention_score, _ = self.multi_head_attention_1(x, x, x , target_mask)\n",
    "    attention_score = nn.Dropout(self.dropout)(attention_score)\n",
    "\n",
    "    # Add and layer norm 1\n",
    "    x = self.layer_norm_1(x + attention_score)\n",
    "\n",
    "    # Multi-Head attention and Dropout\n",
    "    cross_attention_score, _ = self.cross_attention(x, encoder_output, encoder_output, encoder_mask)\n",
    "    cross_attention_score = nn.Dropout(self.dropout)(cross_attention_score)\n",
    "\n",
    "    # Add and layer norm 2\n",
    "    x = self.layer_norm_2(x + cross_attention_score) \n",
    "\n",
    "    # Feed forward and Dropout\n",
    "    ff_output = nn.Dropout(self.dropout)(self.feed_forward(x))\n",
    "\n",
    "    # Add and layer norm 3\n",
    "    x = self.layer_norm_3(x + ff_output)\n",
    "           \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation 4\n",
    "\n",
    "4. After the encoder processes or encodes the source sentence, **Decoder** operation begins. The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
    "\n",
    "When it is said that the model does not attend to subsequent positions means that if we are translating, for example, from Spanish to English the sentence \"Me gusta estudiar\" and in the first iteration we get the token \"I\" which is related to \"Me\" then the Decoder must attend only to these two tokens and not to the rest of the translation \"like to study\" which has not yet been generated, basically we are preventing the model from cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  \"\"\"\n",
    "  A Transformer decoder with multiple layers (`DecoderLayer`) that applies self-attention and cross-attention mechanisms. \n",
    "  Takes encoded input and generates final sequence outputs. Includes layer normalization for stability.\n",
    "\n",
    "  Args:\n",
    "  - d_model (int): Dimension of embeddings.\n",
    "  - n_layers (int): Number of decoder layers.\n",
    "  - d_ff (int): Feed-forward hidden layer size.\n",
    "  - num_heads (int): Attention heads.\n",
    "  - dropout (float): Dropout rate.\n",
    "  \"\"\"\n",
    "  def __init__(self, d_model:int=512, n_layers:int=6, d_ff:int=2048, \n",
    "               num_heads:int=8, dropout:int=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.layers = nn.ModuleList(DecoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(n_layers))\n",
    "    self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x, encoder_output, encoder_mask = None, target_mask = None):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    x: the input of the encoder, these are the translations so far\n",
    "    ouput_encoder: will be the output of the enconder to do cross attention\n",
    "    encoder_mask: the mask used in the encoder to not attend padding\n",
    "    target_mask: the mask used by the decoder to not attend to subsequent words\n",
    "    \"\"\"\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_output, encoder_mask, target_mask)\n",
    "\n",
    "    return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation 5\n",
    "\n",
    "5. At the end when the encoder and decoder have done their job, we do a linear projection. \n",
    "\n",
    "Three additional comments is that (i) the softmax layer is not implemented in the model because it can be implemented at the time of training, so the model is a little more general to be applied in different tasks, (ii) in the embedding layers, we multiply those weights by âˆšdmodel y (iii) dropout [33] is applied at the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_drop = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  \"\"\"\n",
    "  Some Information about Transformer\n",
    "  \n",
    "  params:\n",
    "  source_vocab_size: The size (tokens) of input sentences\n",
    "  target_vocab_size: The size (tokens) of the output sentences\n",
    "  d_model: The size of the word embeddings to use\n",
    "  n_layers: The number of enconder-decoder layers to use\n",
    "  d_ff: Number of neurons in the inner layer of feed forward sublayer\n",
    "  num_heads: The number of heads in the self-attention module\n",
    "  max_seq_len: The maximum number of tokens that the model can receive\n",
    "  dropout: Dropout probability \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, source_vocab_size:int, target_vocab_size:int, d_model:int=512, n_layers:int=6, \n",
    "               d_ff:int=2048, num_heads:int=8, max_seq_len:int=128, dropout:int=0.1, device:str='cpu'):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.d_model = d_model\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.encoder_embedding = nn.Embedding(source_vocab_size, d_model)\n",
    "    self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "    self.positional_encoding = PositionalEncoding(max_seq_len, d_model, device) # dropout\n",
    "    self.encoder = Encoder(d_model, n_layers, d_ff, num_heads, dropout)\n",
    "    self.decoder = Decoder(d_model, n_layers, d_ff, num_heads, dropout)\n",
    "    self.output_layer = nn.Linear(d_model, target_vocab_size) \n",
    "\n",
    "  def forward(self, source, target):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    source: input language sentence\n",
    "    target: output language sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Encoder and decoder masks\n",
    "    source_mask, target_mask = self.mask(source, target)\n",
    "\n",
    "    # 2. Encoder embedding and positional encoding\n",
    "    source = self.encoder_embedding(source) * math.sqrt(self.d_model) # Like in the paper, in the embedding layers, we multiply those weights by âˆšdmodel.\n",
    "    source = nn.Dropout(self.dropout)(self.positional_encoding(source))\n",
    "\n",
    "    # 3. Encoder\n",
    "    encoder_output = self.encoder(source, source_mask)\n",
    "\n",
    "    # 1 and 2. Decoder embedding and positional encoding\n",
    "    target = self.decoder_embedding(target) * math.sqrt(self.d_model)\n",
    "    target = nn.Dropout(self.dropout)(self.positional_encoding(target))\n",
    "\n",
    "    # 4. Decoder\n",
    "    output = self.decoder(target, encoder_output, source_mask, target_mask)\n",
    "\n",
    "    return self.output_layer(output) # 5. Linear projection\n",
    "\n",
    "  def mask(self, source, target):\n",
    "    \"\"\"\n",
    "    This function will allow us to pay attention only to the tokens we have seen.\n",
    "\n",
    "    Modify the self-attention sub-layer in the decoder stack to prevent positions\n",
    "    from attending to subsequent positions. This masking, combined with fact that the output \n",
    "    embeddings are offset by one position, ensures that the predictions for position i can \n",
    "    depend only on the known outputs at positions less than i. It is also used to prevent our\n",
    "    model from not paying attention to padding tokens. \n",
    "    \"\"\"\n",
    "    \n",
    "    source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "    target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "    size = target.size(1)\n",
    "    no_mask = torch.tril(torch.ones((1, size, size), device=self.device)).bool()\n",
    "    target_mask = target_mask & no_mask\n",
    "    return source_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Test the model to see if it gives the expected result.\n",
    "\n",
    "# We assume that the input and output sentence is 10 words long.\n",
    "seq_len_source = 10 \n",
    "seq_len_target = 10\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# We assume that the number of words in the source and target vocabulary are 50.\n",
    "source_vocab_size_test = 50\n",
    "target_vocab_size_test = 50\n",
    "\n",
    "source = torch.randint(1, source_vocab_size_test, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size_test, (batch_size, seq_len_target))\n",
    "model = Transformer(source_vocab_size_test, target_vocab_size_test)\n",
    "\n",
    "output = model(source, target)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "â”œâ”€Embedding: 1-1                                   25,600\n",
       "â”œâ”€Embedding: 1-2                                   25,600\n",
       "â”œâ”€PositionalEncoding: 1-3                          --\n",
       "â”œâ”€Encoder: 1-4                                     --\n",
       "â”‚    â””â”€ModuleList: 2-1                             --\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-1                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-1           1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-2                    1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-3      2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-4                    1,024\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-2                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-5           1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-6                    1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-7      2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-8                    1,024\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-3                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-9           1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-10                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-11     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-12                   1,024\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-4                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-13          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-14                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-15     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-16                   1,024\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-5                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-17          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-18                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-19     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-20                   1,024\n",
       "â”‚    â”‚    â””â”€EncoderLayer: 3-6                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-21          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-22                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-23     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-24                   1,024\n",
       "â”‚    â””â”€LayerNorm: 2-2                              1,024\n",
       "â”œâ”€Decoder: 1-5                                     --\n",
       "â”‚    â””â”€ModuleList: 2-3                             --\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-7                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-25          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-26                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-27          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-28                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-29     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-30                   1,024\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-8                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-31          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-32                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-33          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-34                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-35     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-36                   1,024\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-9                      --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-37          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-38                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-39          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-40                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-41     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-42                   1,024\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-10                     --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-43          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-44                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-45          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-46                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-47     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-48                   1,024\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-11                     --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-49          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-50                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-51          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-52                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-53     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-54                   1,024\n",
       "â”‚    â”‚    â””â”€DecoderLayer: 3-12                     --\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-55          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-56                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€MultiHeadAttention: 4-57          1,050,624\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-58                   1,024\n",
       "â”‚    â”‚    â”‚    â””â”€PositionWiseFeedForward: 4-59     2,099,712\n",
       "â”‚    â”‚    â”‚    â””â”€LayerNorm: 4-60                   1,024\n",
       "â”‚    â””â”€LayerNorm: 2-4                              1,024\n",
       "â”œâ”€Linear: 1-6                                      25,650\n",
       "===========================================================================\n",
       "Total params: 44,217,394\n",
       "Trainable params: 44,217,394\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references\n",
    "\n",
    "- Pepe Cantoral, Ph.D. (2024, June 17). Transformers - Attention is all you need - Parte 1 [Video]. YouTube. https://www.youtube.com/watch?v=Bh22yyEJFak\n",
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017, June 12). Attention is all you need. arXiv.org. https://arxiv.org/abs/1706.03762\n",
    "\n",
    "You can find an application of this architecture [here](https://www.kaggle.com/code/guillepinto/transformers-from-zero-attention-is-all-you-need)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
